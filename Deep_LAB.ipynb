{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IUffyOOK-3q",
        "outputId": "14c67222-4ef0-4125-bb71-4e3cc59f7324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "XaWdYZg5LEv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models import squeezenet1_1\n",
        "import torch.functional as F\n",
        "from io import open\n",
        "import os\n",
        "from PIL import Image\n",
        "import pathlib\n",
        "import glob\n",
        "import cv2"
      ],
      "metadata": {
        "id": "mRXN9pxFSChU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking fot device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "zMVKyrbhLEx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsmUleqZLE0d",
        "outputId": "c77af4cc-33d0-42dd-fa02-58e3ea06a204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforms\n",
        "transformer=transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((150,150)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(), #changes the color from 0-255 to 0-1\n",
        "        transforms.Normalize([0.5,0.5,0.5], #normalizee from 0-1 to -1-1, formula to create new pixules (x-mean)/std\n",
        "                             [0.5,0.5,0.5])\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "t91e5k41LE21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZBoZLTYLE48",
        "outputId": "a926bc88-905b-4b0a-f064-649a5e71d817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Path\n",
        "train_path = '/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN'\n",
        "test_path = '/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TEST'"
      ],
      "metadata": {
        "id": "PX0ejyrPLE7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DataLoader\n",
        "train_loader=DataLoader(\n",
        "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
        "    batch_size=50, shuffle=True\n",
        ")\n",
        "test_loader=DataLoader(\n",
        "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
        "    batch_size=50, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "Zc30oi3PLE-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#catagories\n",
        "root=pathlib.Path(train_path)\n",
        "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])"
      ],
      "metadata": {
        "id": "YOU_HqBDLTUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6Lv166ALTXD",
        "outputId": "dcf5f841-047f-4de4-c2c8-655f01b7c3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['downdog', 'goddess', 'plank', 'tree', 'warrior2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN Network\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self,num_classes=5):\n",
        "        super(ConvNet,self).__init__()\n",
        "\n",
        "        #Output size after convolution filter\n",
        "        #((w-f+2P)/s) +1\n",
        "\n",
        "        #Input shape= (256,3,150,150)\n",
        "\n",
        "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
        "        #Shape= (50,12,150,150)\n",
        "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
        "        #Shape= (50,12,150,150)\n",
        "        self.relu1=nn.ReLU()\n",
        "        #Shape= (50,12,150,150)\n",
        "\n",
        "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
        "        #Reduce the image size be factor 2\n",
        "        #Shape= (50,12,75,75)\n",
        "\n",
        "\n",
        "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
        "        #Shape= (50,20,75,75)\n",
        "        self.relu2=nn.ReLU()\n",
        "        #Shape= (50,20,75,75)\n",
        "\n",
        "\n",
        "\n",
        "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
        "        #Shape= (50,32,75,75)\n",
        "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
        "        #Shape= (50,32,75,75)\n",
        "        self.relu3=nn.ReLU()\n",
        "        #Shape= (50,32,75,75)\n",
        "\n",
        "\n",
        "        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n",
        "\n",
        "\n",
        "\n",
        "        #Feed forwad function\n",
        "\n",
        "    def forward(self,input):\n",
        "        output=self.conv1(input)\n",
        "        output=self.bn1(output)\n",
        "        output=self.relu1(output)\n",
        "\n",
        "        output=self.pool(output)\n",
        "\n",
        "        output=self.conv2(output)\n",
        "        output=self.relu2(output)\n",
        "\n",
        "        output=self.conv3(output)\n",
        "        output=self.bn3(output)\n",
        "        output=self.relu3(output)\n",
        "\n",
        "\n",
        "            #Above output will be in matrix form, with shape (256,32,75,75)\n",
        "\n",
        "        output=output.view(-1,32*75*75)\n",
        "\n",
        "\n",
        "        output=self.fc(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "IMLU66pkLTZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet(num_classes=5).to(device)"
      ],
      "metadata": {
        "id": "acOo9cMnLTbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizer and loss function\n",
        "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
        "loss_function=nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "VJKK01zVLTdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=10"
      ],
      "metadata": {
        "id": "_R8iB9XpLTf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating the size of training and testing images\n",
        "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
        "test_count=len(glob.glob(test_path+'/**/*.jpg'))"
      ],
      "metadata": {
        "id": "gM-9X5utLTjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_count,test_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-gHBlUPLeLT",
        "outputId": "ad654bd2-1ef0-49e6-c2b3-773356ce4a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "999 420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "metadata": {
        "id": "0dTvsWLmLeNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Model training and saving best model\n",
        "\n",
        "best_accuracy=0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    #Evaluation and training on training dataset\n",
        "    model.train()\n",
        "    train_accuracy=0.0\n",
        "    train_loss=0.0\n",
        "\n",
        "    for i, (images,labels) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            images=Variable(images.cuda())\n",
        "            labels=Variable(labels.cuda())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #outputs=model(images)\n",
        "        outputs=model(images)\n",
        "        loss=loss_function(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        train_loss+= loss.cpu().data*images.size(0)\n",
        "        _,prediction=torch.max(outputs.data,1)\n",
        "\n",
        "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
        "\n",
        "    train_accuracy=train_accuracy/train_count\n",
        "    train_loss=train_loss/train_count\n",
        "\n",
        "\n",
        "    # Evaluation on testing dataset\n",
        "    model.eval()\n",
        "\n",
        "    test_accuracy=0.0\n",
        "    for i, (images,labels) in enumerate(test_loader):\n",
        "\n",
        "        outputs=model(images)\n",
        "        _,prediction=torch.max(outputs.data,1)\n",
        "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
        "\n",
        "    test_accuracy=test_accuracy/test_count\n",
        "\n",
        "\n",
        "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
        "\n",
        "    #Save the best model\n",
        "    if test_accuracy>best_accuracy:\n",
        "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
        "        best_accuracy=test_accuracy\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOrhSm5XLeP9",
        "outputId": "7d362b0a-6ac8-4c29-9d9d-41e57ac237fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/PIL/Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Train Loss: tensor(23.6499) Train Accuracy: 0.3123123123123123 Test Accuracy: 0.6642857142857143\n",
            "Epoch: 1 Train Loss: tensor(5.2355) Train Accuracy: 0.6576576576576577 Test Accuracy: 0.6452380952380953\n",
            "Epoch: 2 Train Loss: tensor(2.3573) Train Accuracy: 0.8138138138138138 Test Accuracy: 0.7857142857142857\n",
            "Epoch: 3 Train Loss: tensor(0.9198) Train Accuracy: 0.9309309309309309 Test Accuracy: 0.7738095238095238\n",
            "Epoch: 4 Train Loss: tensor(0.6199) Train Accuracy: 0.9619619619619619 Test Accuracy: 0.7571428571428571\n",
            "Epoch: 5 Train Loss: tensor(1.0111) Train Accuracy: 0.9369369369369369 Test Accuracy: 0.8166666666666667\n",
            "Epoch: 6 Train Loss: tensor(0.3286) Train Accuracy: 1.014014014014014 Test Accuracy: 0.8571428571428571\n",
            "Epoch: 7 Train Loss: tensor(0.2549) Train Accuracy: 1.039039039039039 Test Accuracy: 0.8428571428571429\n",
            "Epoch: 8 Train Loss: tensor(0.2014) Train Accuracy: 1.038038038038038 Test Accuracy: 0.7928571428571428\n",
            "Epoch: 9 Train Loss: tensor(0.0682) Train Accuracy: 1.0660660660660661 Test Accuracy: 0.8476190476190476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------"
      ],
      "metadata": {
        "id": "4YaKlm9hQ0I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jmd_imagescraper"
      ],
      "metadata": {
        "id": "mxZp82_WLeST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea13588-caa5-4788-82d4-56d0cb503daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jmd_imagescraper.core import * # dont't worry, it's designed to work with import *\n",
        "\n",
        "duckduckgo_search(root, \"yoga_dataset\", \"yoga warrior2\", max_results=10)\n",
        "duckduckgo_search(root, \"yoga_dataset\", \"yoga goddess\", max_results=10)\n",
        "duckduckgo_search(root, \"yoga_dataset\", \"yoga plank\", max_results=10)\n",
        "duckduckgo_search(root, \"yoga_dataset\", \"yoga tree\", max_results=10)\n",
        "duckduckgo_search(root, \"yoga_dataset\", \"yoga downdog\", max_results=10)"
      ],
      "metadata": {
        "id": "GMXaPbupLeT8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "3eb57dd1-6359-40de-8d7d-4196d372c842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duckduckgo search: yoga warrior2\n",
            "Downloading results into /content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='10' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [10/10 00:03&lt;00:00 Images downloaded]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duckduckgo search: yoga goddess\n",
            "Downloading results into /content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='10' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [10/10 00:02&lt;00:00 Images downloaded]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duckduckgo search: yoga plank\n",
            "Downloading results into /content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='10' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [10/10 00:02&lt;00:00 Images downloaded]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duckduckgo search: yoga tree\n",
            "Downloading results into /content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='10' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [10/10 00:02&lt;00:00 Images downloaded]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duckduckgo search: yoga downdog\n",
            "Downloading results into /content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='10' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [10/10 00:02&lt;00:00 Images downloaded]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset/041_03b2a077.jpg'),\n",
              " PosixPath('/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset/042_13d84224.jpg'),\n",
              " PosixPath('/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset/043_12dc7875.jpg'),\n",
              " PosixPath('/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset/044_9f5b5932.jpg'),\n",
              " PosixPath('/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset/045_f536bfec.jpg'),\n",
              " PosixPath('/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset/046_91762918.jpg'),\n",
              " PosixPath('/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset/047_e499caf4.jpg'),\n",
              " PosixPath('/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset/048_22e3018a.jpg'),\n",
              " PosixPath('/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset/049_83d9122d.jpg'),\n",
              " PosixPath('/content/gdrive/MyDrive/YOGA/content/cleaned/DATASET/TRAIN/yoga_dataset/050_1f94ab46.jpg')]"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ZIP_NAME = \"imges\" # maybe change this?\n",
        "\n",
        "!rm -f {ZIP_NAME}"
      ],
      "metadata": {
        "id": "m_oTFtvJRAA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#categories\n",
        "root=pathlib.Path(train_path)\n",
        "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
      ],
      "metadata": {
        "id": "kz1UhA_fRAFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint=torch.load('best_checkpoint.model')\n",
        "model=ConvNet(num_classes=5)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "W-o7-O-MRAHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc2c22c-a227-4481-d84f-d09e5fbf4a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNet(\n",
              "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu1): ReLU()\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu3): ReLU()\n",
              "  (fc): Linear(in_features=180000, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction function\n",
        "def prediction(img_path,transformer):\n",
        "\n",
        "    image=Image.open(img_path)\n",
        "\n",
        "    image_tensor=transformer(image).float()\n",
        "\n",
        "\n",
        "    image_tensor=image_tensor.unsqueeze_(0)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        image_tensor.cuda()\n",
        "\n",
        "    input=Variable(image_tensor)\n",
        "\n",
        "\n",
        "    output=model(input)\n",
        "\n",
        "    index=output.data.numpy().argmax()\n",
        "\n",
        "    pred=classes[index]\n",
        "\n",
        "    return pred\n",
        ""
      ],
      "metadata": {
        "id": "FyVvShalRAKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction('/content/gdrive/MyDrive/00000010.jpg',transformer)"
      ],
      "metadata": {
        "id": "_oM9JHAlLeXU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cff3339a-3e74-4cde-e926-854b5197ee59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tree'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    }
  ]
}